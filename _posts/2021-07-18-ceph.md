---
layout: post
title: Ceph
author: Robert Paciorek
tags:
- debian
---

Ceph umożliwia tworzenie klastrów storagowych, czyli zapewnianie jednej spójnej przestrzeni dla danych w oparciu o dyski udostępniane przez wiele różnych komputerów.
Ceph oferuje 3 metody dostępu do tej przestrzeni:

* posixowy system plików
* urządzenia blokowe
* dostęp obiektowy

## Instalacja

Debian dostarcza pakiety z Ceph, jednak jest to typowo wersja trochę starsza. Dlatego jeżeli zależy nam na najnowszej wersji możemy użyć repozytoriów oferowanych przez projekt Ceph:

	wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -
	echo 'deb https://download.ceph.com/debian-octopus/ buster main' > /etc/apt/sources.list.d/ceph.list
	aptitude update

Ceph składa się z kilku komponentów:

* MGR (ManaGeR) – umożliwia zarządzanie klastrem, sprawdzanie jego stanu, itd.
* MON (MONitor) – odpowiada za przechowywanie stanu klastra, stanowi krytyczny komponent dla działania klastra;
  klaster z jednym monitorem będzie działać poprawnie, jednak w celu zapewnienia wysokiej niezawodności zaleca się stosowanie co najmniej 3 niezależnych (nie zainstalowanych na tej samej maszynie fizycznej, itd.) monitorów
* ODS (Object Storage Daemon) – odpowiada za przechowywanie danych na lokalnym storage (dostępnym z serwera na którym działa) i udostępnianie go poprzez sieć dla innych maszyn w klastrze
* MDS (MetaData Server) – serwer metadanych dla dostępu poprzez posixowy system plików (pozwala na wykonywanie poleceń typu ls bez obciążania OSD)

Mogą one być instalowane indywidualnie (pakiety `ceph-mgr`, `ceph-mon`, `ceph-osd`, `ceph-mds`), tylko na tych maszynach na których są potrzebne.
Nie ma jednak przeszkód aby na wszystkich maszynach zepewnić jednakowy obraz systemu zawierający wszystkie komponenty (nawet gdy część z nich nie będzie na danej maszynie używana) poprzez:

	aptitude install ceph ceph-mds uuid-runtime


## Serwery „zarządzające”

**Uwaga:** Poniższe instrukcje zakładają że mon, mgr i mds działają na jednej maszynie.
Wydaje się to być dobrym rozwiązaniem dla pierwszego niewielkiego klastra testowego, jednak przy większych / poważniejszych instalacjach zasadne może być rozdzielenie tych funki na różne serwery.


### konfiguracja klastra i uruchomienie monitora

Pierwszym krokiem w tworzeniu klastra jest przygotowanie jego konfiguracji:

	UUID=`uuidgen`
	CLUSTERNAME=ceph
	MON_NAME=`hostname -s`
	MON_IP="2001:0db8:0:1234::1"
	CLUSTER_NET="2001:0db8:0:1234::0/64"

	cat <<EOF > /etc/ceph/$CLUSTERNAME.conf
	[global]
	fsid = $UUID
	mon initial members = $MON_NAME
	mon host = $MON_IP
	public network = $CLUSTER_NET
	ms bind ipv4 = false
	ms bind ipv6 = true
	EOF

(Uwaga: powyższy przykład używa adresów IPv6. Możliwe jest używanie tak IPv6, jak i IPv4, jednak nie obu równocześnie – dokładnie jedna z opcji `ms bind ipv*` musi być ustawiona na `true`.)

Następnie generujemy klucze które będą używane do autoryzacji elementów klastra:

	ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
	ceph-authtool --create-keyring /etc/ceph/$CLUSTERNAME.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
	ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'
	ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/$CLUSTERNAME.client.admin.keyring
	ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
	chown ceph:ceph /tmp/ceph.mon.keyring

I tworzymy mapę klastra, klaster oraz uruchamiamy pierwszy monitor:

	monmaptool --create --add $MON_NAME $MON_IP --fsid $UUID /tmp/monmap
	sudo -u ceph mkdir /var/lib/ceph/mon/$CLUSTERNAME-$MON_NAME
	sudo -u ceph ceph-mon --cluster $CLUSTERNAME --mkfs -i $MON_NAME --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring
	
	systemctl start "ceph-mon@$MON_NAME"
	# gdy jest problem można uruchomić "z palca":
	# /usr/bin/ceph-mon -f --cluster $CLUSTERNAME --id plgrid-ceph0 --setuser ceph --setgroup ceph

W tym momencie możemy sprawdzić poprawność działania klastra poleceniem:

	ceph --cluster $CLUSTERNAME -s

Powinno wypisać informacje o klastrze, usługach, itd.


### uruchomienie menagera

	sudo -u ceph mkdir /var/lib/ceph/mgr/$CLUSTERNAME-$MON_NAME
	sudo -u ceph ceph auth --cluster $CLUSTERNAME get-or-create mgr.$MON_NAME mon 'allow profile mgr' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mgr/$CLUSTERNAME-$MON_NAME/keyring
	
	systemctl start "ceph-mgr@$MON_NAME"


### uruchomienie MDSa

	sudo -u ceph mkdir -p /var/lib/ceph/mds/$CLUSTERNAME-$MON_NAME
	sudo -u ceph ceph-authtool --create-keyring /var/lib/ceph/mds/$CLUSTERNAME-$MON_NAME/keyring --gen-key -n mds.$MON_NAME
	sudo -u ceph ceph auth add mds.$MON_NAME osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/$CLUSTERNAME-$MON_NAME/keyring
	
	echo "[mds.$MON_NAME]" >> /etc/ceph/$CLUSTERNAME.conf
	echo "host = $MON_IP" >> /etc/ceph/$CLUSTERNAME.conf
	
	systemctl start "ceph-mds@$MON_NAME"


## Serwery storagowe

### uruchomienie OSD

Na maszyny na których mają działać OSD kopiujemy (z maszyny na której uruchamialiśmy pierwszy monitor) pliki `/etc/ceph/ceph.conf` i `/var/lib/ceph/bootstrap-osd/ceph.keyring` do takich samych lokalizacji.
Następnie dla każdego dysku, który chcemy udostępnić w ramach Ceph'a wykonujemy (na maszynie dysponującej tym dyskiem):

	ceph-volume lvm create --data /SCIEZKA/DO/URZĄDZENIA/BLOKOWEGO/DYSKU

Na przykład:

	ceph-volume lvm create --data /dev/sdb3


## Administracja klastrem Ceph

### tworzenie i montowanie cephfs

Z poziomu maszyny na której działa mgr wykonujemy polecenia:
	ceph fs volume create ceph0

	ceph fs authorize ceph0 client.gw / rw > ceph.client.gw.keyring
	
Plik `ceph.client.gw.keyring` kopiujemy do `/etc/ceph/` na maszynę która ma montować system plików i tam:

	chmod 600 /etc/ceph/ceph.client.gw.keyring
	mkdir /mnt/ceph0
	echo '[$MON_IP]:/ /mnt/ceph0 ceph name=gw 0 1' >> /etc/fstab
	mount /mnt/ceph0

Z poziomu maszyny na której działa mgr możemy także dostosować poziom redundancji (ilość tworzonych kopi) dla danego systemu plików:
	ceph osd dump | grep --color 'replicated size'
	ceph osd pool set cephfs.ceph0.meta size 2
	ceph osd pool set cephfs.ceph0.data size 2


### tworzenie i używanie pool dla object storage

	# wylistowanie istniejących pool'i
	ceph osd lspools
	
	# utworzenie nowego
	ceph osd pool create NOWY_POOL
	
	# ustawienie poziomu redundancji
	ceph osd pool set NOWY_POOL size 2
	
	# wylistowanie uprawnień dostępu
	ceph auth ls
	# dodanie praw R/W dla klienta NOWY_KLIENT do NOWY_POOL
	ceph auth add client.NOWY_KLIENT mon 'allow r' osd 'allow rw pool=NOWY_POOL'
	
	# listowanie zawartości pool'a NOWY_POOL
	rados -p NOWY_POOL ls
	
	# pobranie obiektu IDENTYFIKATOR_W_POOL z pool'a NOWY_POOL do pliku PLIK
	rados -p NOWY_POOL get IDENTYFIKATOR_W_POOL PLIK


## Więcej informacji

* [Oficjalna dokumentacja Ceph](https://docs.ceph.com/en/latest/start/intro/)
